# Paper
## MOE

[Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the em algorithm.](https://github.com/fltenwall/Paper/blob/main/MOE/Jordan%2C%20M.%20I.%20and%20Jacobs%2C%20R.%20A.%20(1994).%20Hierarchical%20mixtures%20of%20experts%20and%20the%20em%20algorithm..pdf)

[Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J.](https://github.com/fltenwall/Paper/blob/main/MOE/Shazeer%2C%20N.%2C%20Mirhoseini%2C%20A.%2C%20Maziarz%2C%20K.%2C%20Davis%2C%20A.%2C%20Le%2C%20Q.%2C%20Hinton%2C%20G.%20and%20Dean%2C%20J..pdf)

[Towards Understanding Mixture of Experts in Deep](https://github.com/fltenwall/Paper/blob/main/MOE/Towards%20Understanding%20Mixture%20of%20Experts%20in%20Deep.pdf)

[NIPS-2009-hierarchical-mixture-of-classification-experts-uncovers-interactions-between-brain-regions-Paper.pdf](https://github.com/fltenwall/Paper/blob/main/MOE/NIPS-2009-hierarchical-mixture-of-classification-experts-uncovers-interactions-between-brain-regions-Paper.pdf)

[MOE/A-Survey-on-Mixture-of-Experts(2024.7).pdf](https://github.com/fltenwall/Paper/blob/main/MOE/A-Survey-on-Mixture-of-Experts.pdf)
